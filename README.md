# Deep Learning Concepts and Implementations

This repository contains comprehensive notes and code implementations related to various deep learning concepts. The notes are organized in the `meta` folder, and the code files are stored in the `code` folder.

## Table of Contents

1. [Simple Neuron](#simple-neuron)
2. [Neural Network Structure](#neural-network-structure)
3. [Types of Activation Functions](#types-of-activation-functions)
4. [Layers and Types](#layers-and-types)
5. [Advantages of Deep Neural Networks (DNN) over Machine Learning (ML)](#advantages-of-dnn-over-ml)
6. [DNN Example: Logic Gates](#dnn-example-logic-gates)
7. [Hyperparameters](#hyperparameters)
   - [Alpha (Learning Rate)](#alpha-learning-rate)
   - [Batch Size](#batch-size)
8. [Optimizers](#optimizers)
9. [Regularizations](#regularizations)
10. [Neural Network Numerical: Full Forward and Backward Propagation Solved](#nn-numerical-full-forward-and-backward-solved)
11. [Convolutional Neural Networks (CNN)](#cnn)
    - [Types of Convolutions](#types-of-convolutions)
    - [CNN Formulas](#cnn-formulas)
12. [Transfer Learning and Fine-Tuning](#transfer-learning-and-fine-tuning)
13. [Region-based Convolutional Neural Networks (R-CNN) and YOLO](#rcnn-and-yolo)
    - [Non-Maximum Suppression](#non-maximum-suppression)
14. [Recurrent Neural Networks (RNN)](#rnn)
    - [LSTM](#lstm)
    - [GRU](#gru)
15. [RNN Numerical](#rnn-numerical)
16. [Attention Layers](#attention-layers)
17. [Transformers](#transformers)

## Simple Neuron
- Description and implementation of a basic neuron.

## Neural Network Structure
- Overview of neural network architecture and layers.

## Types of Activation Functions
- Explanation and examples of different activation functions such as ReLU, Sigmoid, Tanh, etc.

## Layers and Types
- Detailed descriptions of different layers (Dense, Convolutional, Recurrent, etc.) and their types.

## Advantages of DNN over ML
- Discussion on why deep neural networks often outperform traditional machine learning algorithms.

## DNN Example: Logic Gates
- Implementation of logic gates using deep neural networks.

## Hyperparameters
### Alpha (Learning Rate)
- Explanation of the learning rate and its impact on training.

### Batch Size
- Discussion on batch size and how it affects model performance and training time.

## Optimizers
- Overview of different optimization algorithms (SGD, Adam, RMSprop, etc.).

## Regularizations
- Techniques to prevent overfitting, such as L1, L2 regularization, and dropout.

## Neural Network Numerical: Full Forward and Backward Propagation Solved
- Step-by-step numerical example solving both forward and backward propagation in a neural network.

## Convolutional Neural Networks (CNN)
- Introduction to CNNs and their applications.
### Types of Convolutions
- Explanation of various convolution types (standard, dilated, transposed, etc.).

### CNN Formulas
- Mathematical formulas related to CNN operations.

## Transfer Learning and Fine-Tuning
- Concepts of transfer learning and how to fine-tune pre-trained models for specific tasks.

## Region-based Convolutional Neural Networks (R-CNN) and YOLO
- Comparison and implementation details of R-CNN and YOLO for object detection.
### Non-Maximum Suppression
- Explanation of non-maximum suppression in object detection.

## Recurrent Neural Networks (RNN)
- Overview of RNNs and their use cases.
### LSTM
- Detailed description and implementation of Long Short-Term Memory networks.

### GRU
- Detailed description and implementation of Gated Recurrent Units.

## RNN Numerical
- Numerical example solving an RNN problem.

## Attention Layers
- Introduction to attention mechanisms in neural networks.

## Transformers
- Overview of transformer models and their applications in NLP and beyond.

## Meta Folder
- Contains handwritten notes for all the topics listed above.

## Code Folder
- Contains code files implementing the concepts and examples discussed.

Feel free to explore the repository and make use of the resources provided. Contributions and suggestions are welcome!
